# Critical Issues in Information Systems Research: Large Language Models 

## What is this?

This repository (soon) contains materials for the PhD course *Critical Issues in Information Systems Research: Large Language Models*, which will be lectured at Aalto University during spring 2025. 

## Course description

Due to the rapid technological developments in artificial intelligence during the past three years, an increasing number of researchers from different fields are pursuing research directly or indirectly related to generative AI and large language models. However, less emphasis has been placed on building a rigorous foundation by reading seminal works, which have mainly appeared in leading machine learning conferences (and arXiv). To bridge this gap, this course provides a starting point to reading, understanding and following technical AI literature. More specifically, the course covers papers such as the ones that introduced Deepseek-r1, Llama 2 and GPT-3. 

This is a highly interactive course, where the emphasis is on reading the required papers in advance before the lectures, in order to be able to discuss the topics with the lecturer and peers. Participants will also get to write and present a research proposal, which ideally can be used as a basis for a thesis or publication. 


## Format
In-person lecture, exercise and paper discussion sessions on:
* 14.4, 10.00 â€“ 12.00
* 15.4, 10.00 â€“ 12.00
* 16.4, 10.00 â€“ 12.00

Optional feedback and consultation session for final projects:
* 28.4, 10 â€“ 12.00

Project presentations
* 26.5, 9.00 â€“ 12.00

Deadline for submitting the written component of the final project:
* 30.5, 11.59

## Learning goals
* Understanding and being able to explain the principles of how reinforcement learning, generative AI and large language models work
* Understanding how progress in AI is measured an evaluated 
* Knowing where and how to follow the development of AI research

## Evaluation
* 20% active participation during sessions
* 20% pen and paper quiz on key topics
* 60% final project (research plan & outline for a conference paper OR MSc thesis)

## Prerequisites 
There are no formal prerequisites for this course, but having completed any introductory machine learning and deep learning courses gives a significant advantage. E.g. the course at Aalto Deep Learning with Python D. While this course contains no programming assignments, conceptually the topics are easier to understand with previous hands-on experience.

## Reading list (to be updated)


### Mandatory 
@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}

@article{touvron2023llama,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@article{guo2025deepseek,
  title={Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning},
  author={Guo, Daya and Yang, Dejian and Zhang, Haowei and Song, Junxiao and Zhang, Ruoyu and Xu, Runxin and Zhu, Qihao and Ma, Shirong and Wang, Peiyi and Bi, Xiao and others},
  journal={arXiv preprint arXiv:2501.12948},
  year={2025}
}


### Recommended
@article{bommasani2021opportunities,
  title={On the opportunities and risks of foundation models},
  author={Bommasani, Rishi and Hudson, Drew A and Adeli, Ehsan and Altman, Russ and Arora, Simran and von Arx, Sydney and Bernstein, Michael S and Bohg, Jeannette and Bosselut, Antoine and Brunskill, Emma and others},
  journal={arXiv preprint arXiv:2108.07258},
  year={2021}
}

@inproceedings{bender2021dangers,
  title={On the dangers of stochastic parrots: Can language models be too big?ðŸ¦œ},
  author={Bender, Emily M and Gebru, Timnit and McMillan-Major, Angelina and Shmitchell, Shmargaret},
  booktitle={Proceedings of the 2021 ACM conference on fairness, accountability, and transparency},
  pages={610--623},
  year={2021}
}

@article{dean2022golden,
  title={A golden decade of deep learning: Computing systems \& applications},
  author={Dean, Jeffrey},
  journal={Daedalus},
  volume={151},
  number={2},
  pages={58--74},
  year={2022},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~â€¦}
}






