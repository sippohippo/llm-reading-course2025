# Critical Issues in Information Systems Research: Large Language Models 

## What is this?

This repository (soon) contains materials for the PhD course *Critical Issues in Information Systems Research: Large Language Models*, which will be lectured at Aalto University during spring 2025. 

## Course description

Due to the rapid technological developments in artificial intelligence during the past three years, an increasing number of researchers from different fields are pursuing research directly or indirectly related to generative AI and large language models. However, less emphasis has been placed on building a rigorous foundation by reading seminal works, which have mainly appeared in leading machine learning conferences (and arXiv). To bridge this gap, this course provides a starting point to reading, understanding and following technical AI literature. More specifically, the course covers papers such as the ones that introduced Deepseek-r1, Llama 2 and GPT-3. 

This is a highly interactive course, where the emphasis is on reading the required papers in advance before the lectures, in order to be able to discuss the topics with the lecturer and peers. Participants will also get to write and present a research proposal, which ideally can be used as a basis for a thesis or publication. 

## Schedule
In-person lecture, exercise and paper discussion sessions on:
* 14.4, 10.00 â€“ 12.00 (Discussing paper [[1]](#1))
* 15.4, 10.00 â€“ 12.00 (Discussing paper [[2]](#2), [[3]](#3))
* 16.4, 10.00 â€“ 12.00 (Quiz, discussing paper [[4]](#4))

Optional feedback and consultation session for final projects:
* 28.4, 10.00 â€“ 12.00

Project presentations
* 26.5, 9.00 â€“ 12.00

Deadline for submitting the written component of the final project:
* 30.5, 11.59

## Learning goals
* Understanding and being able to explain the principles of how reinforcement learning, generative AI and large language models work
* Understanding how progress in AI is measured an evaluated 
* Knowing where and how to follow the development of AI research

## Evaluation
* 20% active participation during sessions
* 20% pen and paper quiz on key topics
* 60% final project (research plan & outline for a conference paper OR MSc thesis)

## Prerequisites 
There are no formal prerequisites for this course, but having completed any introductory machine learning and deep learning courses gives a significant advantage. E.g. the course at Aalto Deep Learning with Python D. While this course contains no programming assignments, conceptually the topics are easier to understand with previous hands-on experience.

## Reading list


### Mandatory 
<a id="1">[1]</a> 
Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., ... & Amodei, D. (2020). Language models are few-shot learners. Advances in neural information processing systems, 33, 1877-1901. [Link](https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html)

<a id="1">[2]</a> 
Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., ... & Lowe, R. (2022). Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35, 27730-27744. [Link](https://proceedings.neurips.cc/paper_files/paper/2022/hash/b1efde53be364a73914f58805a001731-Abstract-Conference.html)

<a id="1">[3]</a> 
Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., ... & Scialom, T. (2023). Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288. 
<https://doi.org/10.48550/arXiv.2307.09288>

<a id="1">[4]</a> 
Guo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R., ... & He, Y. (2025). Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948.
<https://doi.org/10.48550/arXiv.2501.12948>

### Recommended
<a id="1">[5]</a> 
Bommasani, R., Hudson, D. A., Adeli, E., Altman, R., Arora, S., von Arx, S., ... & Liang, P. (2021). On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258. <https://doi.org/10.48550/arXiv.2108.07258>

<a id="1">[6]</a> 
Bender, E. M., Gebru, T., McMillan-Major, A., & Shmitchell, S. (2021, March). On the dangers of stochastic parrots: Can language models be too big?ðŸ¦œ. In Proceedings of the 2021 ACM conference on fairness, accountability, and transparency(pp. 610-623). <https://doi.org/10.1145/3442188.3445922>

<a id="1">[7]</a> 
Dean, J. (2022). A golden decade of deep learning: Computing systems & applications. Daedalus, 151(2), 58-74. <https://doi.org/10.1162/daed_a_01900>

### Other materials

[Let's build GPT: from scratch, in code, spelled out](https://www.youtube.com/watch?v=kCc8FmEb1nY), a great video explaining how GPT-models approximately work.


